{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found\n"
     ]
    }
   ],
   "source": [
    "a = \"asd:\"\n",
    "if a[-1] == \":\":\n",
    "    print('found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Python3 code for preprocessing text\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('D:\\Contegris work\\Datasets\\Both\\\\both_language.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>Text_Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36840</td>\n",
       "      <td>Molvi ab na wo area raha na wo inches</td>\n",
       "      <td>Roman Urdu</td>\n",
       "      <td>molvi ab na wo area raha na wo inches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45611</td>\n",
       "      <td>Workshop a rae hai</td>\n",
       "      <td>Roman Urdu</td>\n",
       "      <td>workshop a rae hai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38142</td>\n",
       "      <td>Street children ko sarkon par nahi rehna chahiye.</td>\n",
       "      <td>Roman Urdu</td>\n",
       "      <td>street children ko sarkon par nahi rehna chahiye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13986</td>\n",
       "      <td>I have vowed to only purchase high-waisted bot...</td>\n",
       "      <td>English</td>\n",
       "      <td>i have vowed to only purchase highwaisted bott...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28748</td>\n",
       "      <td>Ek Sakhun aur ke phir Tarz e Takallum tera,,,H...</td>\n",
       "      <td>Roman Urdu</td>\n",
       "      <td>ek sakhun aur ke phir tarz e takallum terahusu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46780</th>\n",
       "      <td>885</td>\n",
       "      <td>I saw this skirt online and went to the store ...</td>\n",
       "      <td>English</td>\n",
       "      <td>i saw this skirt online and went to the store ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46781</th>\n",
       "      <td>10230</td>\n",
       "      <td>Better than the picture - i like the layered l...</td>\n",
       "      <td>English</td>\n",
       "      <td>better than the picture  i like the layered lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46782</th>\n",
       "      <td>6772</td>\n",
       "      <td>I tried this on in white and considered whethe...</td>\n",
       "      <td>English</td>\n",
       "      <td>i tried this on in white and considered whethe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46783</th>\n",
       "      <td>19706</td>\n",
       "      <td>Always looking for sweaters that are wearable ...</td>\n",
       "      <td>English</td>\n",
       "      <td>always looking for sweaters that are wearable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46784</th>\n",
       "      <td>10728</td>\n",
       "      <td>Beautiful shirt! i recently purchased this on ...</td>\n",
       "      <td>English</td>\n",
       "      <td>beautiful shirt i recently purchased this on s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46785 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               text  \\\n",
       "0           36840              Molvi ab na wo area raha na wo inches   \n",
       "1           45611                                 Workshop a rae hai   \n",
       "2           38142  Street children ko sarkon par nahi rehna chahiye.   \n",
       "3           13986  I have vowed to only purchase high-waisted bot...   \n",
       "4           28748  Ek Sakhun aur ke phir Tarz e Takallum tera,,,H...   \n",
       "...           ...                                                ...   \n",
       "46780         885  I saw this skirt online and went to the store ...   \n",
       "46781       10230  Better than the picture - i like the layered l...   \n",
       "46782        6772  I tried this on in white and considered whethe...   \n",
       "46783       19706  Always looking for sweaters that are wearable ...   \n",
       "46784       10728  Beautiful shirt! i recently purchased this on ...   \n",
       "\n",
       "         language                                       Text_Cleaned  \n",
       "0      Roman Urdu              molvi ab na wo area raha na wo inches  \n",
       "1      Roman Urdu                                 workshop a rae hai  \n",
       "2      Roman Urdu   street children ko sarkon par nahi rehna chahiye  \n",
       "3         English  i have vowed to only purchase highwaisted bott...  \n",
       "4      Roman Urdu  ek sakhun aur ke phir tarz e takallum terahusu...  \n",
       "...           ...                                                ...  \n",
       "46780     English  i saw this skirt online and went to the store ...  \n",
       "46781     English  better than the picture  i like the layered lo...  \n",
       "46782     English  i tried this on in white and considered whethe...  \n",
       "46783     English  always looking for sweaters that are wearable ...  \n",
       "46784     English  beautiful shirt i recently purchased this on s...  \n",
       "\n",
       "[46785 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45526"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(data['Text_Cleaned'])\n",
    "# summarize\n",
    "(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode document\n",
    "vector = vectorizer.transform(data['Text_Cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#using 30% of the data for testing, this will be revised once we do not get the desired accuracy\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"Text_Cleaned\"], data[\"language\"], test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.transform(data['Text_Cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============Using Naive Bayes==============\n",
      " \n",
      "Accuracy : 0.9830073741583841\n",
      "F1-score: 0.9830113248725316\n",
      "Precision: 0.9835829658730872\n",
      "Recall: 0.9830073741583841\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Read the data into a Pandas DataFrame\n",
    "data = pd.read_csv(\"D:\\Contegris work\\Datasets\\Both\\\\both_language.csv\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"language\"], test_size=0.2)\n",
    "\n",
    "# Convert the text to a numerical representation using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Multinomial Naive Bayes model\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "\n",
    "print(\"\\n==============Using Naive Bayes==============\\n \")\n",
    "print(\"Accuracy :\", accuracy)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============Using Random Forest==============\n",
      " \n",
      "Accuracy: 0.9957251255744363\n",
      "F1-score: 0.9957252697333521\n",
      "Precision: 0.995738516771057\n",
      "Recall: 0.9957251255744363\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Read the data into a Pandas DataFrame\n",
    "# data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"language\"], test_size=0.2)\n",
    "\n",
    "# Convert the text to a numerical representation using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Calculate the F1-score, precision and recall\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "\n",
    "print(\"\\n==============Using Random Forest==============\\n \")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lanugage_loaded(text):\n",
    "    \n",
    "    a = vectorizer.transform([text])\n",
    "    pred = clf.predict(a)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============Using SVM==============\n",
      " \n",
      "Accuracy: 0.9980763065084963\n",
      "F1-score: 0.9980763190439647\n",
      "Precision: 0.9980764228595892\n",
      "Recall: 0.9980763065084963\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Read the data into a Pandas DataFrame\n",
    "# data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"language\"], test_size=0.2)\n",
    "\n",
    "# Convert the text to a numerical representation using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Calculate the F1-score, precision and recall\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "\n",
    "print(\"\\n==============Using SVM==============\\n \")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============Using Logistic regression==============\n",
      " \n",
      "Accuracy: 0.996580100459549\n",
      "F1-score: 0.9965800795219371\n",
      "Precision: 0.9965815156417304\n",
      "Recall: 0.996580100459549\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Read the data into a Pandas DataFrame\n",
    "# data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"language\"], test_size=0.2)\n",
    "\n",
    "# Convert the text to a numerical representation using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a RandomForestClassifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Calculate the F1-score, precision and recall\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "\n",
    "print(\"\\n==============Using Logistic regression==============\\n \")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lanugage_loaded(text):\n",
    "\n",
    "    a = vectorizer.transform([text])\n",
    "    pred = clf.predict(a)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['English'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_lanugage_loaded(\"this is very bad product I dont like it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['English'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = vectorizer.transform(['this is very bad product I dont like it'])\n",
    "pred = clf.predict(a)\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in data[\"language\"]:\n",
    "    if i == \"Roman Urdu\":\n",
    "        y.append(0)\n",
    "    else:\n",
    "        y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Contegris work\\lazy_predict_lang.ipynb Cell 15\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Contegris%20work/lazy_predict_lang.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m Tokenizer()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Contegris%20work/lazy_predict_lang.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m tokenizer\u001b[39m.\u001b[39mfit_on_texts(X_train)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Contegris%20work/lazy_predict_lang.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m X_train \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mtexts_to_sequences(X_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Contegris%20work/lazy_predict_lang.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m X_test \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_sequences(X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Contegris%20work/lazy_predict_lang.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Pad the sequences to the same length\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\keras\\preprocessing\\text.py:337\u001b[0m, in \u001b[0;36mTokenizer.texts_to_sequences\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtexts_to_sequences\u001b[39m(\u001b[39mself\u001b[39m, texts):\n\u001b[0;32m    326\u001b[0m   \u001b[39m\"\"\"Transforms each text in texts to a sequence of integers.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \n\u001b[0;32m    328\u001b[0m \u001b[39m  Only top `num_words-1` most frequent words will be taken into account.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[39m      A list of sequences.\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtexts_to_sequences_generator(texts))\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\keras\\preprocessing\\text.py:366\u001b[0m, in \u001b[0;36mTokenizer.texts_to_sequences_generator\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    365\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manalyzer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 366\u001b[0m     seq \u001b[39m=\u001b[39m text_to_word_sequence(\n\u001b[0;32m    367\u001b[0m         text, filters\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilters, lower\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlower, split\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit)\n\u001b[0;32m    368\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     seq \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manalyzer(text)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\keras\\preprocessing\\text.py:74\u001b[0m, in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m     72\u001b[0m   input_text \u001b[39m=\u001b[39m input_text\u001b[39m.\u001b[39mlower()\n\u001b[1;32m---> 74\u001b[0m translate_dict \u001b[39m=\u001b[39m {c: split \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m filters}\n\u001b[0;32m     75\u001b[0m translate_map \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\u001b[39m.\u001b[39mmaketrans(translate_dict)\n\u001b[0;32m     76\u001b[0m input_text \u001b[39m=\u001b[39m input_text\u001b[39m.\u001b[39mtranslate(translate_map)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\keras\\preprocessing\\text.py:74\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m     72\u001b[0m   input_text \u001b[39m=\u001b[39m input_text\u001b[39m.\u001b[39mlower()\n\u001b[1;32m---> 74\u001b[0m translate_dict \u001b[39m=\u001b[39m {c: split \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m filters}\n\u001b[0;32m     75\u001b[0m translate_map \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\u001b[39m.\u001b[39mmaketrans(translate_dict)\n\u001b[0;32m     76\u001b[0m input_text \u001b[39m=\u001b[39m input_text\u001b[39m.\u001b[39mtranslate(translate_map)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Read the data into a Pandas DataFrame\n",
    "# data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"],y , test_size=0.2)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to the same length\n",
    "max_length = max([len(x) for x in X_train])\n",
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length)\n",
    "\n",
    "# Convert the labels to categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_length))\n",
    "model.add(LSTM(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "score, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73e43853552d6fc356b2c05e3ac4c7e6c4dba3aed2cbc26a744008cb0f5a4666"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
